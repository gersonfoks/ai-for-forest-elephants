{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.dataset import AudioDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pandas import DataFrame\n",
    "\n",
    "%load_ext autoreload\n",
    "# We use the development dataset for this example:\n",
    "base_dir = '../data/training'\n",
    "metadata_df: DataFrame | None = pd.read_csv('../data/training/metadata.tsv', sep='\\t', names=['file_reference', 'start_time', 'end_time', 'label'])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "from src.data.split import compare_wav_file_refs, split_list\n",
    "import functools\n",
    "from src.data.dataset import get_file_names\n",
    "\n",
    "wav_refs, labels = get_file_names(base_dir)\n",
    "split_percentage = 0.8\n",
    "\n",
    "gunshots = [wav_ref for wav_ref in wav_refs if wav_ref.label == 'Gunshot']\n",
    "rumbles = [wav_ref for wav_ref in wav_refs if wav_ref.label == 'Rumble']\n",
    "\n",
    "gunshots_sorted = sorted(gunshots, key=functools.cmp_to_key(compare_wav_file_refs))\n",
    "rumbles_sorted = sorted(rumbles, key=functools.cmp_to_key(compare_wav_file_refs))\n",
    "\n",
    "# Split the data\n",
    "train_gunshots, val_gunshots = split_list(gunshots_sorted, split_percentage)\n",
    "train_rumbles, val_rumbles = split_list(rumbles_sorted, split_percentage)\n",
    "\n",
    "# Merge the data\n",
    "training_wav_files = train_gunshots + train_rumbles\n",
    "val_wav_files = val_gunshots + val_rumbles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "val_dataset = AudioDataset('', metadata_df, wav_files = val_wav_files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the AST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerso\\anaconda3\\envs\\ai-for-elephants\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\Python Projects\\\\Fruitpunch\\\\Elephants\\\\model-exploration\\\\checkpoints_ast\\\\example_simple_transformer_best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mE:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mPython Projects\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mFruitpunch\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mElephants\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mmodel-exploration\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mcheckpoints_ast\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mexample_simple_transformer_best.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\projects\\fruitpunch\\model-exploration\\src\\models\\utils.py:38\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_model\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AbstractModel:\n\u001B[0;32m     35\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;124;03m        Loads the model from the path\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m---> 38\u001B[0m     saved_model \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m     config \u001B[38;5;241m=\u001B[39m saved_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     40\u001B[0m     model \u001B[38;5;241m=\u001B[39m get_model(config)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai-for-elephants\\lib\\site-packages\\torch\\serialization.py:791\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    789\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 791\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    793\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    794\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    795\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    796\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai-for-elephants\\lib\\site-packages\\torch\\serialization.py:271\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    272\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    273\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai-for-elephants\\lib\\site-packages\\torch\\serialization.py:252\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 252\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'E:\\\\Python Projects\\\\Fruitpunch\\\\Elephants\\\\model-exploration\\\\checkpoints_ast\\\\example_simple_transformer_best.pt'"
     ]
    }
   ],
   "source": [
    "from src.models.utils import load_model\n",
    "\n",
    "model = load_model('E:\\Python Projects\\Fruitpunch\\Elephants\\model-exploration\\checkpoints_ast\\example_simple_transformer_best.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the HTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import htsat\n",
    "from hts_transformer.htsat import HTSAT_Swin_Transformer\n",
    "import hts_transformer.config as config\n",
    "\n",
    "model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=1,\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config = config,\n",
    "    depths = config.htsat_depth,\n",
    "    embed_dim = config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "\n",
    "# Load from checkpoint\n",
    "checkpoint = torch.load('E:\\Python Projects\\Fruitpunch\\Elephants\\model-exploration\\checkpoints\\epoch=33-step=27302.ckpt', map_location=torch.device('cuda'))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.data.dataset import collate_fn\n",
    "# Create a dataloader to iterate over the dataset\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "from src.metrics import get_metrics\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_metrics_for_rumbles(model, dataloader):\n",
    "    model = model.to('cuda')\n",
    "    metrics  = {\n",
    "        'acc': torchmetrics.Accuracy(task='binary').to('cuda'),\n",
    "        'f1': torchmetrics.F1Score(task='binary', average='macro').to('cuda'),\n",
    "        'precision': torchmetrics.Precision(task='binary', average='macro').to('cuda'),\n",
    "        'recall': torchmetrics.Recall(task='binary', average='macro').to('cuda'),\n",
    "\n",
    "    }\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        audio, labels = batch\n",
    "        audio = audio.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        predictions = model(audio)  # Swap to model.infer(audio) for HTS\n",
    "\n",
    "        for key, metric in metrics.items():\n",
    "            metric( predictions[:, 0], labels[:, 0])\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "from src.metrics import get_metrics\n",
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_metrics_for_all(model, dataloader):\n",
    "    model = model.to(device)\n",
    "    metrics  = {\n",
    "        'acc': torchmetrics.Accuracy(task='binary').to(device),\n",
    "        'f1': torchmetrics.F1Score(task='binary', average='macro').to(device),\n",
    "        'precision': torchmetrics.Precision(task='binary', average='macro').to(device),\n",
    "        'recall': torchmetrics.Recall(task='binary', average='macro').to(device),\n",
    "\n",
    "    }\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        audio, labels = batch\n",
    "        audio = audio.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = model(audio) # Swap to model.infer(audio) for HTS\n",
    "\n",
    "        for key, metric in metrics.items():\n",
    "            metric( predictions, labels)\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = calculate_metrics_for_all(model, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_results = {key: m.compute().to('cpu') for key, m in metrics.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics = calculate_metrics_for_all(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric_results = {key: m.compute().to('cpu') for key, m in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': tensor(0.8483), 'f1': tensor(0.7650), 'precision': tensor(0.7073), 'recall': tensor(0.8330)}\n"
     ]
    }
   ],
   "source": [
    "print(metric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}